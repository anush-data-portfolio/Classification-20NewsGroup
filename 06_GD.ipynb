{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement seab (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for seab\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4 --quiet\n",
    "!pip3 install lxml --quiet\n",
    "!pip3 install requests --quiet\n",
    "!pip3 install nltk --quiet\n",
    "!pip3 install spacy --quiet\n",
    "!pip3 install seab\n",
    "!python3 -m spacy download en_core_web_sm --quiet\n",
    "!pip3 install imblearn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1s/d6cby_d90fsfb94qvy0p0ttm0000gn/T/ipykernel_58168/1668287111.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "  corect_logprobs = -np.log(probabilities[range(len(X_batch)), y_batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 8.1382, Validation Accuracy: 0.0544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1s/d6cby_d90fsfb94qvy0p0ttm0000gn/T/ipykernel_58168/1668287111.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "  corect_logprobs = -np.log(probabilities[range(len(X_batch)), y_batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 9.5185, Validation Accuracy: 0.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1s/d6cby_d90fsfb94qvy0p0ttm0000gn/T/ipykernel_58168/1668287111.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "  corect_logprobs = -np.log(probabilities[range(len(X_batch)), y_batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 15.8970, Validation Accuracy: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1s/d6cby_d90fsfb94qvy0p0ttm0000gn/T/ipykernel_58168/1668287111.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "  corect_logprobs = -np.log(probabilities[range(len(X_batch)), y_batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 16.2406, Validation Accuracy: 0.0820\n",
      "Epoch 5/100, Loss: 11.2610, Validation Accuracy: 0.0867\n",
      "Epoch 6/100, Loss: 6.3146, Validation Accuracy: 0.0971\n",
      "Epoch 7/100, Loss: 5.3928, Validation Accuracy: 0.1045\n",
      "Epoch 8/100, Loss: 8.3881, Validation Accuracy: 0.1159\n",
      "Epoch 9/100, Loss: 5.6712, Validation Accuracy: 0.1225\n",
      "Epoch 10/100, Loss: 11.3951, Validation Accuracy: 0.1340\n",
      "Epoch 11/100, Loss: 4.8385, Validation Accuracy: 0.1459\n",
      "Epoch 12/100, Loss: 3.9764, Validation Accuracy: 0.1557\n",
      "Epoch 13/100, Loss: 10.9784, Validation Accuracy: 0.1647\n",
      "Epoch 14/100, Loss: 6.7317, Validation Accuracy: 0.1724\n",
      "Epoch 15/100, Loss: 5.8250, Validation Accuracy: 0.1828\n",
      "Epoch 16/100, Loss: 0.8139, Validation Accuracy: 0.1936\n",
      "Epoch 17/100, Loss: 7.3661, Validation Accuracy: 0.2042\n",
      "Epoch 18/100, Loss: 3.4389, Validation Accuracy: 0.2156\n",
      "Epoch 19/100, Loss: 2.3894, Validation Accuracy: 0.2231\n",
      "Epoch 20/100, Loss: 2.3719, Validation Accuracy: 0.2329\n",
      "Epoch 21/100, Loss: 3.9062, Validation Accuracy: 0.2422\n",
      "Epoch 22/100, Loss: 2.3989, Validation Accuracy: 0.2523\n",
      "Epoch 23/100, Loss: 3.0598, Validation Accuracy: 0.2629\n",
      "Epoch 24/100, Loss: 5.9909, Validation Accuracy: 0.2708\n",
      "Epoch 25/100, Loss: 2.3946, Validation Accuracy: 0.2814\n",
      "Epoch 26/100, Loss: 2.2133, Validation Accuracy: 0.2907\n",
      "Epoch 27/100, Loss: 1.5411, Validation Accuracy: 0.3019\n",
      "Epoch 28/100, Loss: 1.3945, Validation Accuracy: 0.3141\n",
      "Epoch 29/100, Loss: 1.5436, Validation Accuracy: 0.3255\n",
      "Epoch 30/100, Loss: 2.6430, Validation Accuracy: 0.3324\n",
      "Epoch 31/100, Loss: 2.2738, Validation Accuracy: 0.3427\n",
      "Epoch 32/100, Loss: 2.6751, Validation Accuracy: 0.3499\n",
      "Epoch 33/100, Loss: 2.8565, Validation Accuracy: 0.3597\n",
      "Epoch 34/100, Loss: 2.6839, Validation Accuracy: 0.3692\n",
      "Epoch 35/100, Loss: 1.7852, Validation Accuracy: 0.3788\n",
      "Epoch 36/100, Loss: 2.7454, Validation Accuracy: 0.3867\n",
      "Epoch 37/100, Loss: 2.1022, Validation Accuracy: 0.3923\n",
      "Epoch 38/100, Loss: 1.8353, Validation Accuracy: 0.4013\n",
      "Epoch 39/100, Loss: 1.8866, Validation Accuracy: 0.4101\n",
      "Epoch 40/100, Loss: 1.7995, Validation Accuracy: 0.4164\n",
      "Epoch 41/100, Loss: 2.5815, Validation Accuracy: 0.4260\n",
      "Epoch 42/100, Loss: 2.6682, Validation Accuracy: 0.4353\n",
      "Epoch 43/100, Loss: 1.6409, Validation Accuracy: 0.4432\n",
      "Epoch 44/100, Loss: 1.5731, Validation Accuracy: 0.4509\n",
      "Epoch 45/100, Loss: 1.8611, Validation Accuracy: 0.4597\n",
      "Epoch 46/100, Loss: 1.1788, Validation Accuracy: 0.4676\n",
      "Epoch 47/100, Loss: 2.6248, Validation Accuracy: 0.4761\n",
      "Epoch 48/100, Loss: 1.9333, Validation Accuracy: 0.4865\n",
      "Epoch 49/100, Loss: 1.1522, Validation Accuracy: 0.4928\n",
      "Epoch 50/100, Loss: 1.1195, Validation Accuracy: 0.5040\n",
      "Epoch 51/100, Loss: 2.0844, Validation Accuracy: 0.5127\n",
      "Epoch 52/100, Loss: 1.3458, Validation Accuracy: 0.5207\n",
      "Epoch 53/100, Loss: 1.6211, Validation Accuracy: 0.5294\n",
      "Epoch 54/100, Loss: 2.1245, Validation Accuracy: 0.5350\n",
      "Epoch 55/100, Loss: 1.8298, Validation Accuracy: 0.5432\n",
      "Epoch 56/100, Loss: 0.7028, Validation Accuracy: 0.5509\n",
      "Epoch 57/100, Loss: 1.0281, Validation Accuracy: 0.5594\n",
      "Epoch 58/100, Loss: 0.9712, Validation Accuracy: 0.5674\n",
      "Epoch 59/100, Loss: 1.8085, Validation Accuracy: 0.5737\n",
      "Epoch 60/100, Loss: 1.5670, Validation Accuracy: 0.5812\n",
      "Epoch 61/100, Loss: 0.6750, Validation Accuracy: 0.5862\n",
      "Epoch 62/100, Loss: 0.3657, Validation Accuracy: 0.5870\n",
      "Epoch 63/100, Loss: 2.0097, Validation Accuracy: 0.5920\n",
      "Epoch 64/100, Loss: 0.9627, Validation Accuracy: 0.5958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the newsgroup dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Preprocess the data\n",
    "vectorizer = CountVectorizer( stop_words='english')\n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize model parameters and hyperparameters\n",
    "np.random.seed(42)\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "learning_rate = 0.01\n",
    "reg_strength = 0.01\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Initialize weights and biases\n",
    "W = np.random.randn(num_features, num_classes)\n",
    "b = np.zeros(num_classes)\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Extract a batch of data\n",
    "        X_batch = X_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        logits = np.dot(X_batch, W) + b\n",
    "        probabilities = softmax(logits)\n",
    "\n",
    "\n",
    "        # Compute the loss, enure division by zero does not occur\n",
    "        corect_logprobs = -np.log(probabilities[range(len(X_batch)), y_batch])\n",
    "        loss = np.sum(corect_logprobs) / len(X_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Compute the gradient\n",
    "        error = probabilities\n",
    "        error[range(len(X_batch)), y_batch] -= 1\n",
    "\n",
    "        gradient_W = np.dot(X_batch.T, error) / len(X_batch)\n",
    "        gradient_b = np.sum(error, axis=0) / len(X_batch)\n",
    "\n",
    "        # Add regularization\n",
    "        gradient_W += reg_strength * W\n",
    "\n",
    "        # Update parameters using gradient descent\n",
    "        W -= learning_rate * gradient_W\n",
    "        b -= learning_rate * gradient_b\n",
    "\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    val_logits = np.dot(X_val, W) + b\n",
    "    val_probabilities = softmax(val_logits)\n",
    "    val_predictions = np.argmax(val_probabilities, axis=1)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    # Add convergence criteria (e.g., stop if validation loss stops decreasing)\n",
    "\n",
    "# Evaluate the model on the test set or use it for predictions\n",
    "test_logits = np.dot(X_val, W) + b\n",
    "test_probabilities = softmax(test_logits)\n",
    "test_predictions = np.argmax(test_probabilities, axis=1)\n",
    "test_accuracy = accuracy_score(y_val, test_predictions)\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Newsgroup-h-5eaQBu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
